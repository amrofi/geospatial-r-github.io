---
title: ''
---

<div style="margin-bottom:10px;">
</div>
```{r echo=FALSE, out.width='100%', fig.align="center"}
knitr::include_graphics("E:\\GitHub\\geospatial-r-github\\Image\\empty_banner.png")
```
<div style="margin-bottom:20px;">
</div>

<div style="margin-bottom:20px;">
</div>

```{r echo=FALSE, out.width='100%', fig.align="center"}
knitr::include_graphics('E:\\GitHub\\geospatial-r-github\\Image\\DNN_h2o.png')
```

<div style="margin-bottom:30px;">
</div>

# Deep Neural Network - Supervised Image Classification in H20 R
<div style="margin-bottom:20px;">
</div>

**Deep Neural Networks (or Deep Dearning)** is based on a multi-layer, feed-forward artificial neural network that is trained with stochastic gradient descent using back-propagation. The network can contain many hidden layers consisting of neurons with activation functions. Advanced features such as adaptive learning rate, rate annealing, momentum training, dropout, L1 or L2 regularization, checkpointing, and grid search enable high predictive accuracy. Each computer node trains a copy of the global model parameters on its local data with multi-threading (asynchronously) and contributes periodically to the global model via model averaging across the network.

In this tutorial will show how to implement a Deep Neural Network for pixel based supervised classification of Sentinel-2 multispectral images using the [H20](http://h2o-release.s3.amazonaws.com/h2o/rel-lambert/5/docs-website/Ruser/Rinstall.html) package in R.


[H2O is an open source, in-memory, distributed, fast, and scalable machine learning and predictive analytics platform that allows you to build machine learning models on big data and provides easy productionalization of those models in an enterprise environment](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html). It's core code is written in Java and can read data in parallel from a distributed cluster and also from local cluster. H2O allows access to all the capabilities of H2O from an external program or script via JSON over HTTP. The Rest API is used by H2O's [web interface (Flow UI)](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html), [R binding (H2O-R)](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html), and [Python binding (H2O-Python)](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html). Requirement and installation steps in  R can be found [here](http://h2o-release.s3.amazonaws.com/h2o/rel-wheeler/2/index.html).

First, we will split the “point_data” into a training set (75% of the data), a validation set (12%) and a test set (13%) data.  The validation data set will be used to optimize the model parameters during the training process. The model’s performance will be tested with the test data set. Finally, we will predict land use classes using a grid data set.


#### Load packages 

```{r message=F, warning=F}
library(rgdal)         # spatial data processing
library(raster)        # raster processing
library(plyr)          # data manipulation 
library(dplyr)         # data manipulation 
library(RStoolbox)     # plotting raster data 
library(ggplot2)       # plotting 
library(RColorBrewer)  # Color
library(sp)            # Spatial data
library(ggplot2)       # Plotting
```

<div style="margin-bottom:20px;">
</div>

The data could be available for download from [here](https://www.dropbox.com/s/bupqmsfyr2tk1yg/Data_RS_DNN.7z?dl=0).

<div style="margin-bottom:20px;">
</div>

```{r}
# Define data folder
dataFolder<-"D:\\Dropbox\\Spatial Data Analysis and Processing in R\\DATA_RS_DNN\\"
```


#### Load point and grid data

```{r}
point<-read.csv(paste0(dataFolder,".\\Sentinel_2\\point_data.csv"), header = T)
grid<-read.csv(paste0(dataFolder,".\\Sentinel_2\\prediction_grid_data.csv"), header = T)
```

##### Creat data frames  

```{r}
point.data<-cbind(point[c(3:13)])
grid.data<-grid[c(4:13)]
grid.xy<-grid[c(3,1:2)]
```

#### Install H2O

```{r}
#install.packages("h20")
```

#### Start and Initialize  H20 local cluster

```{r message=F, warning=F,results="hide"}
library(h2o)
localH2o <- h2o.init(nthreads = -1) 
```

#### Import data to H2O cluster

```{r message=F, warning=F,results="hide"}
df<-  as.h2o(point.data)
grid<- as.h2o(grid.data)
```

#### Split data into train, validation and test dataset

```{r}
splits <- h2o.splitFrame(df, c(0.75,0.125), seed=1234)
train  <- h2o.assign(splits[[1]], "train.hex") # 75%
valid  <- h2o.assign(splits[[2]], "valid.hex") # 12%
test   <- h2o.assign(splits[[3]], "test.hex")  # 13%
```

#### Create response and features data sets

```{r message=F, warning=F,results="hide"}
y <- "Class"
x <- setdiff(names(train), y)
```

<div style="margin-bottom:20px;">
</div>


## **Tuning and Optimizations parameters:** 


More details about the of Tuning and Optimization parameters of the H20 Deep Neural Network for supervised classification can be found [here](http://docs.h2o.ai/h2o-tutorials/latest-stable/tutorials/deeplearning/index.html)


### Grid Search for hyperprameters

We can use the h2o.grid() function to perform a Random Grid Search (RGS). We could also test all possible combinations of parameters with Cartesian Grid or exhaustive search, but RGS is much faster when we have a large number of possible combinations and usually finds sufficiently accurate models.For RGS, we first define a set of hyper-parameters and search criteria to fine-tune our models. Because there are many hyper-parameters, each with a range of possible values, we want to find an (ideally) optimal combination to maximize our model's accuracy. 

**Activation Functions**

**Rectifier:** is the default activation function. It is the fastest and most versatile option. It can lead to instability though and tends to be lower in accuracy.
**Tanh**: The hyperbolic tangent is a scaled and shifted variant of the sigmoid activation function. It can take on values from -1 to 1 and centers around 0. Tanh needs more computational power than e.g. the Rectifier function.
**Maxout**: is an activation function that is the max of the inputs. It is computationally quite demanding but can produce high accuracy models.
**    ...WithDropout:** When we specify with dropout, a random subset of the network is trained and the weights of all sub-networks are averaged. It works together with the parameter hidden_dropout_ratios, which controls the amount of layer neurons that are randomly dropped for each hidden layer. Hidden dropout ratios are useful for preventing overfitting on learned features.

**Hidden layers**

are the most important hyper-parameter to set for deep neural networks, as they specify how many hidden layers and how many nodes per hidden layer the model should learn

**L1 and L2 penalties**

L1: lets only strong weights survive
L2: prevents any single weight from getting too big.

**rho** Adaptive learning rate time decay factor (similarity to prior updates). Defaults to 0.99.
**epsilon** Adaptive learning rate smoothing factor (to avoid divisions by zero and allow progress). Defaults to 1e-08.

**momentum_start** Initial momentum at the beginning of training (try 0.5). Defaults to 0.
**momentum_stable** Final momentum after the ramp is over (try 0.99). Defaults to 0.
**max_w2** Constraint for squared sum of incoming weights per unit (e.g. for Rectifier). Defaults to 3.4028235e+38.


```{r message=F, warning=F}
hyper_params <- list(
                     activation = c("Rectifier", "Maxout", "Tanh",
                                    "RectifierWithDropout", "MaxoutWithDropout",
                                    "TanhWithDropout"),                      
                     hidden = list(c(5, 5, 5, 5, 5), c(10, 10, 10, 10), c(50, 50, 50), c(100, 100, 100)),
                     epochs = c(50, 100, 200),
                     l1 = c(0, 0.00001, 0.0001), 
                     l2 = c(0, 0.00001, 0.0001),
                     rate = c(0, 01, 0.005, 0.001),
                     rate_annealing = c(1e-8, 1e-7, 1e-6),
                     rho = c(0.9, 0.95, 0.99, 0.999),
                     epsilon = c(1e-10, 1e-8, 1e-6, 1e-4),
                     momentum_start = c(0, 0.5),
                     momentum_stable = c(0.99, 0.5, 0),
                     input_dropout_ratio = c(0, 0.1, 0.2),
                     max_w2 = c(10, 100, 1000, 3.4028235e+38)
                     )
hyper_params
```


### Early stopping criteria

**stopping_metric:** metric that we want to use as stopping criterion
**stopping_tolerance and stopping_rounds**: training stops when the the stopping metric does not improve by the stopping tolerance proportion any more (e.g. by 0.05 or 5%) for the number of consecutive rounds defined by stopping rounds.


```{r}
search_criteria <- list(strategy = "RandomDiscrete", 
                        max_models = 200,
                        max_runtime_secs = 900,
                        stopping_tolerance = 0.001,
                        stopping_rounds = 2,
                        seed = 1345767)
```

Now, we can train the model with combinations of hyper-parameters from our specified stopping criteria and hyper-parameter grid.

```{r  message=F, warning=F,results = "hide"}
DNN_grid <- h2o.grid(
                  algorithm="deeplearning",
                  grid_id = "DNN_grid_ID",
                  x= x,
                  y = y,
                  training_frame = train,
                  validation_frame =valid,  
                  stopping_metric = "logloss",
                  nfolds=10,
                  hyper_params = hyper_params,
                  search_criteria = search_criteria,
                  seed = 42)
```

#### Grid parameters

```{r message=F, warning=F, results = "hide"}
grid.dnn <- h2o.getGrid("DNN_grid_ID",sort_by="RMSE",decreasing=FALSE)
```


```{r}
grid.dnn@summary_table[1,]
```

### The Best Model

```{r message=F, warning=F}
best_model <- h2o.getModel(grid.dnn@model_ids[[1]]) ## model with lowest logloss
best_model
```

#### The Best parameters

```{r}
best_params <- best_model@allparameters
```


#### Mean error

```{r}
h2o.mean_per_class_error(best_model, train = TRUE, valid = TRUE, xval = TRUE)
```

#### Scoring history

```{r}
scoring_history<-best_model@model$scoring_history
#scoring_history
#write.csv(scoring_history, "scoring_history_model_02.csv")
```

####  Plot the classification error 

```{r, warning=FALSE, fig.width = 5, fig.height = 5}
plot(best_model,
     timestep = "epochs",
     metric = "classification_error")
```

#### Plot logloss 

```{r, warning=FALSE, fig.width = 5, fig.height = 5}
plot(best_model,
     timestep = "epochs",
     metric = "logloss")
```

####  Cross validation result

```{r}
print(best_model@model$cross_validation_metrics_summary%>%.[,c(1,2)])
#capture.output(print(dl_model@model$cross_validation_metrics_summary%>%.[,c(1,2)]),file =  "DL_CV_model_01.txt")
```

#### Model performance with Test data set
#### Compare the training error with the validation and test set errors

```{r}
h2o.performance(best_model, newdata=train)     ## full train data
h2o.performance(best_model, newdata=valid)     ## full validation data
h2o.performance(best_model, newdata=test)     ## full test data

#capture.output(print(h2o.performance(dl_model,test)),file =  "test_data_model_01.txt")
```

#### Confusion matrix

```{r message=F, warning=F,results="hide"}
train.cf<-h2o.confusionMatrix(best_model)
print(train.cf)
valid.cf<-h2o.confusionMatrix(best_model,valid=TRUE)
print(valid.cf)
test.cf<-h2o.confusionMatrix(best_model,test)
print(test.cf)
#write.csv(train.cf, "CFM_train_model_01.csv")
#write.csv(valid.cf, "CFM_valid_model_01.csv")
#write.csv(test.cf, "CFM_test_moldel_01.csv")
```

#### Grid Prediction

```{r message=F, warning=F,results="hide"}
g.predict = as.data.frame(h2o.predict(object = best_model, newdata = grid))
```

#### Stop h20 cluster

```{r}
h2o.shutdown(prompt=FALSE)
```

#### Extract Prediction Class


```{r message=F, warning=F}
# Extract predicted landuse class
grid.xy$Class<-g.predict$predict  
# Import lnaduse ID file 
ID<-read.csv(paste0(dataFolder,".\\Sentinel_2\\Landuse_ID.csv"), header=T)
# Join landuse ID
grid.new<-join(grid.xy, ID, by="Class", type="inner") 
# Omit missing values
grid.new.na<-na.omit(grid.new)                                                            
```


#### Convert to raster and write

```{r}
x<-SpatialPointsDataFrame(as.data.frame(grid.new.na)[, c("x", "y")], data = grid.new.na)
r <- rasterFromXYZ(as.data.frame(x)[, c("x", "y", "Class_ID")])
```


#### Plot map

```{r,echo=TRUE,fig.align='center',fig.height=5, fig.width=6.5,message=F, warning=F}
# Create color palette
myPalette <- colorRampPalette(c("light grey","burlywood4", "forestgreen","light green", "dodgerblue"))
# Plot Map
LU<-spplot(r,"Class_ID", main="Supervised Image Classification: Deep Learning-H20" , 
      colorkey = list(space="right",tick.number=1,height=1, width=1.5,
              labels = list(at = seq(1,4.8,length=5),cex=1.0,
              lab = c("Road/parking/pavement" ,"Building", "Tree/buses", "Grass", "Water"))),
              col.regions=myPalette,cut=4)
LU
```


#### Write raster

```{r}
# writeRaster(r, filename = paste0(dataFolder,".\\Sentinel_2\\DNN_H20_Landuse.tiff"), "GTiff", overwrite=T)
```


```{r}
rm(list = ls())
```
