algorithm="randomForest",
grid_id = "drf_grid_IDs",
x= x,
y = y,
training_frame = train,
validation_frame =valid,
stopping_metric = "logloss",
nfolds=10,
keep_cross_validation_predictions = TRUE,
hyper_params = drf_hyper_params,
search_criteria = drf_search_criteria,
seed = 42)
# Hyper-parameter
drf_hyper_params <-list(
ntrees  = seq(10, 5000, by = 10),
max_depth=c(10,20,30,40,50),
sample_rate=c(0.7, 0.8, 0.9, 1.0)
)
#  Serach criteria
drf_search_criteria <- list(strategy = "RandomDiscrete",
max_models = 200,
max_runtime_secs = 900,
stopping_tolerance = 0.001,
stopping_rounds = 2,
seed = 1345767)
# Grid Search
drf_grid <- h2o.grid(
algorithm="randomForest",
grid_id = "drf_grid_ID",
x= x,
y = y,
training_frame = train,
validation_frame =valid,
stopping_metric = "logloss",
nfolds=10,
keep_cross_validation_predictions = TRUE,
hyper_params = drf_hyper_params,
search_criteria = drf_search_criteria,
seed = 42)
# RF Grid parameters
drf_get_grid <- h2o.getGrid("drf_grid_ID",sort_by="AUC",decreasing=FALSE)
# RF Grid parameters
drf_get_grid <- h2o.getGrid("drf_grid_ID",sort_by="logloss",decreasing=FALSE)
drf_get_grid@summary_table[1,]
# Hyper-parameter
gbm_hyper_params = list( ntrees = c(100,500, 1000),
max_depth = seq(1,20),
min_rows = c(1,5,10,20,50,100),
learn_rate = seq(0.001,0.01,0.001),
sample_rate = seq(0.3,1,0.05),
col_sample_rate = seq(0.3,1,0.05),
col_sample_rate_per_tree = seq(0.3,1,0.05))
# Serach criteria
gbm_search_criteria <- list(strategy = "RandomDiscrete",
max_models = 200,
max_runtime_secs = 900,
stopping_tolerance = 0.001,
stopping_rounds = 2,
seed = 1345767)
# Grid Search
gbm_grid <- h2o.grid(
algorithm="gbm",
grid_id = "GBM_grid_ID",
x= x,
y = y,
training_frame = train,
validation_frame =valid,
stopping_metric = "logloss",
nfolds=10,
hyper_params = gbm_hyper_params,
search_criteria = gbm_search_criteria,
seed = 42)
gbm_get_grid<- h2o.getGrid("GBM_grid_ID",sort_by="logloss",decreasing=FALSE)
gbm_get_grid@summary_table[1,]
best_gbm <- h2o.getModel(gbm_get_grid@model_ids[[1]]) ## model with lowest logloss
# Hyper-prameter
dnn_hyper_params <- list(
activation = c("Rectifier",
"Maxout",
"Tanh",
"RectifierWithDropout",
"MaxoutWithDropout",
"TanhWithDropout"),
hidden = list(c(5, 5, 5, 5, 5), c(10, 10, 10, 10),
c(50, 50, 50), c(100, 100, 100)),
epochs = c(50, 100, 200),
l1 = c(0, 0.00001, 0.0001),
l2 = c(0, 0.00001, 0.0001),
rate = c(0, 01, 0.005, 0.001),
rate_annealing = c(1e-8, 1e-7, 1e-6),
rho = c(0.9, 0.95, 0.99, 0.999),
epsilon = c(1e-10, 1e-8, 1e-6, 1e-4),
momentum_start = c(0, 0.5),
momentum_stable = c(0.99, 0.5, 0),
input_dropout_ratio = c(0, 0.1, 0.2),
max_w2 = c(10, 100, 1000, 3.4028235e+38)
)
# serach criteris
dnn_search_criteria <- list(strategy = "RandomDiscrete",
max_models = 200,
max_runtime_secs = 900,
stopping_tolerance = 0.001,
stopping_rounds = 2,
seed = 1345767)
# Grid search
DNN_grid <- h2o.grid(
algorithm="deeplearning",
grid_id = "DNN_grid_ID",
x= predictors,
y = response,
training_frame = train,
validation_frame =valid,
stopping_metric = "logloss",
nfolds=10,
keep_cross_validation_predictions = TRUE,
hyper_params = dnn_hyper_params,
search_criteria = dnn_search_criteria,
seed = 42)
# Hyper-prameter
dnn_hyper_params <- list(
activation = c("Rectifier",
"Maxout",
"Tanh",
"RectifierWithDropout",
"MaxoutWithDropout",
"TanhWithDropout"),
hidden = list(c(5, 5, 5, 5, 5), c(10, 10, 10, 10),
c(50, 50, 50), c(100, 100, 100)),
epochs = c(50, 100, 200),
l1 = c(0, 0.00001, 0.0001),
l2 = c(0, 0.00001, 0.0001),
rate = c(0, 01, 0.005, 0.001),
rate_annealing = c(1e-8, 1e-7, 1e-6),
rho = c(0.9, 0.95, 0.99, 0.999),
epsilon = c(1e-10, 1e-8, 1e-6, 1e-4),
momentum_start = c(0, 0.5),
momentum_stable = c(0.99, 0.5, 0),
input_dropout_ratio = c(0, 0.1, 0.2),
max_w2 = c(10, 100, 1000, 3.4028235e+38)
)
# serach criteris
dnn_search_criteria <- list(strategy = "RandomDiscrete",
max_models = 200,
max_runtime_secs = 900,
stopping_tolerance = 0.001,
stopping_rounds = 2,
seed = 1345767)
# Grid search
DNN_grid <- h2o.grid(
algorithm="deeplearning",
grid_id = "DNN_grid_ID",
x= x,
y = y,
training_frame = train,
validation_frame =valid,
stopping_metric = "logloss",
nfolds=10,
keep_cross_validation_predictions = TRUE,
hyper_params = dnn_hyper_params,
search_criteria = dnn_search_criteria,
seed = 42)
# RF Grid parameters
dnn_get_grid <- h2o.getGrid("DNN_grid_ID",sort_by="logloss",decreasing=FALSE)
dnn_get_grid@summary_table[1,]
best_dnn <- h2o.getModel(dnn_get_grid@model_ids[[1]])
# model parmeters
meta_para_rf<-list(ntrees = 200, max_depth = 20, nbins=20)
drf_ensemble <- h2o.stackedEnsemble(
model_id = "stack_drf_IDs",
x= x,
y = y,
training_frame = train,
validation_frame =valid,
base_models = list(best_drf,
best_dnn,
best_gbm),
metalearner_algorithm = "drf",
metalearner_params = meta_para_rf,
metalearner_nfolds = 10,
keep_levelone_frame = TRUE,
seed=123)
# model parmeters
meta_para_rf<-list(ntrees = 200, max_depth = 20, nbins=20)
drf_ensemble <- h2o.stackedEnsemble(
model_id = "stack_drf_IDs",
x= x,
y = y,
training_frame = train,
validation_frame =valid,
base_models = list(
best_dnn,
best_gbm),
metalearner_algorithm = "drf",
metalearner_params = meta_para_rf,
metalearner_nfolds = 10,
keep_levelone_frame = TRUE,
seed=123)
# model parmeters
meta_para_rf<-list(ntrees = 200, max_depth = 20, nbins=20)
drf_ensemble <- h2o.stackedEnsemble(
model_id = "stack_drf_IDs",
x= x,
y = y,
training_frame = train,
validation_frame =valid,
base_models = list(
best_dnn,
best_gbm),
metalearner_algorithm = "drf",
metalearner_params = meta_para_rf,
metalearner_nfolds = 10,
keep_levelone_frame = TRUE,
seed=123)
# model parmeters
meta_para_rf<-list(ntrees = 200, max_depth = 20, nbins=20)
drf_ensemble <- h2o.stackedEnsemble(
model_id = "stack_drf_ID",
x= x,
y = y,
training_frame = train,
validation_frame =valid,
base_models = list(
best_dnn,
best_gbm),
metalearner_algorithm = "drf",
metalearner_params = meta_para_rf,
metalearner_nfolds = 10,
keep_levelone_frame = TRUE,
seed=123)
best_drf <- h2o.getModel(drf_get_grid@model_ids[[1]])
# model parmeters
meta_para_rf<-list(ntrees = 200, max_depth = 20, nbins=20)
drf_ensemble <- h2o.stackedEnsemble(
model_id = "stack_drf_ID",
x= x,
y = y,
training_frame = train,
validation_frame =valid,
base_models = list(best_drf,
best_dnn,
best_gbm),
metalearner_algorithm = "drf",
metalearner_params = meta_para_rf,
metalearner_nfolds = 10,
keep_levelone_frame = TRUE,
seed=123)
# model parmeters
meta_para_rf<-list(ntrees = 200, max_depth = 20, nbins=20)
drf_ensemble <- h2o.stackedEnsemble(
model_id = "stack_drf_IDxx",
x= x,
y = y,
training_frame = train,
validation_frame =valid,
base_models = list(best_drf,
best_dnn,
best_gbm),
metalearner_algorithm = "drf",
metalearner_params = meta_para_rf,
metalearner_nfolds = 10,
keep_levelone_frame = TRUE,
seed=123)
# model parmeters
meta_para_rf<-list(ntrees = 200, max_depth = 20, nbins=20)
drf_ensemble <- h2o.stackedEnsemble(
model_id = "stack_drf_IDxx",
x= x,
y = y,
training_frame = train,
validation_frame =valid,
base_models = list(best_drf,
best_dnn,
best_gbm),
metalearner_algorithm = "drf",
metalearner_params = meta_para_rf,
metalearner_nfolds = 10,
seed=123)
# model parmeters
meta_para_rf<-list(ntrees = 200, max_depth = 20, nbins=20)
drf_ensemble <- h2o.stackedEnsemble(
model_id = "stack_drf_IDa",
x= x,
y = y,
training_frame = train,
validation_frame =valid,
base_models = list(best_drf,
best_dnn,
best_gbm),
metalearner_algorithm = "drf",
metalearner_params = meta_para_rf,
metalearner_nfolds = 10,
seed=123)
# Hyper-parameter
gbm_hyper_params = list( ntrees = c(100,500, 1000),
max_depth = seq(1,20),
min_rows = c(1,5,10,20,50,100),
learn_rate = seq(0.001,0.01,0.001),
sample_rate = seq(0.3,1,0.05),
col_sample_rate = seq(0.3,1,0.05),
col_sample_rate_per_tree = seq(0.3,1,0.05))
# Serach criteria
gbm_search_criteria <- list(strategy = "RandomDiscrete",
max_models = 200,
max_runtime_secs = 900,
stopping_tolerance = 0.001,
stopping_rounds = 2,
seed = 1345767)
# Grid Search
gbm_grid <- h2o.grid(
algorithm="gbm",
grid_id = "GBM_grid_IDx",
x= x,
y = y,
training_frame = train,
validation_frame =valid,
stopping_metric = "logloss",
nfolds=10,
hyper_params = gbm_hyper_params,
keep_cross_validation_predictions = TRUE,
search_criteria = gbm_search_criteria,
seed = 42)
gbm_get_grid<- h2o.getGrid("GBM_grid_IDx",sort_by="logloss",decreasing=FALSE)
gbm_get_grid@summary_table[1,]
best_gbm <- h2o.getModel(gbm_get_grid@model_ids[[1]]) ## model with lowest logloss
# model parmeters
meta_para_rf<-list(ntrees = 200, max_depth = 20, nbins=20)
drf_ensemble <- h2o.stackedEnsemble(
model_id = "stack_drf_IDa",
x= x,
y = y,
training_frame = train,
validation_frame =valid,
base_models = list(best_drf,
best_dnn,
best_gbm),
metalearner_algorithm = "drf",
metalearner_params = meta_para_rf,
metalearner_nfolds = 10,
seed=123)
# model parmeters
meta_para_rf<-list(ntrees = 200, max_depth = 20, nbins=20)
drf_ensemble <- h2o.stackedEnsemble(
model_id = "stack_drf_IDb",
x= x,
y = y,
training_frame = train,
validation_frame =valid,
base_models = list(best_drf,
best_dnn,
best_gbm),
metalearner_algorithm = "drf",
metalearner_params = meta_para_rf,
metalearner_nfolds = 10,
seed=123)
h2o.performance(best_drf, newdata=test)     ## full test data
h2o.performance(best_gbm, newdata=test)     ## full test data
h2o.performance(best_dnn, newdata=test)     ## full test data
h2o.performance(drf_ensemble, newdata=test) ## full test data
h2o.performance(best_drf, newdata=test)     ## full test data
h2o.performance(best_gbm, newdata=test)     ## full test data
h2o.performance(best_dnn, newdata=test)     ## full test data
h2o.performance(drf_ensemble, newdata=test) ## full test data
h2o.confusionMatrix(best_drf,test)
h2o.accuracy(best_drf)
gbm.per<-h2o.performance(best_gbm, newdata=test)     ## full test data
print(gbm.per)
h2o.accuracy(gbm.per)
gbm.per<-h2o.performance(best_gbm, newdata=test)     ## full test data
print(gbm.per)
h2o.giniCoef(gbm.per)
h2o.giniCoef(gbm.per)
h2o.giniCoef(gbm.per)
h2o.mcc(gbm.per)
h2o.F1(gbm.per)
h2o.auc(best_gbm, valid = TRUE)
drf.perf<-h2o.performance(best_drf, newdata=test)             ## full test data
gbm.perf<-h2o.performance(best_gbm, newdata=test)             ## full test data
dnn.perf<-h2o.performance(best_dnn, newdata=test)             ## full test data
ensemble.perf<-h2o.performance(drf_ensemble, newdata=test)    ## full test data
h2o.mean_per_class_error(drf.perf)
h2o.mean_per_class_error(gbm.perf)
h2o.mean_per_class_error(dnn.perf)
h2o.mean_per_class_error(ensemble.perf)
cat('RF Error:',h2o.mean_per_class_error(drf.perf), '\n')
cat('RF Error:',h2o.mean_per_class_error(drf.perf), '\n')
cat('GBM Error:',h2o.mean_per_class_error(gbm.perf), '\n')
cat('DNN Error:',h2o.mean_per_class_error(dnn.perf), '\n')
cat('Ensmeble Error:',h2o.mean_per_class_error(drf.perf), '\n')
cat('RF Log-loss:',h2o.logloss(drf.perf), '\n')
cat('GBM Log-lossr:',h2o.logloss(gbm.perf), '\n')
cat('DNN Log-loss:',h2o.logloss(dnn.perf), '\n')
cat('Ensmeble Log-loss:',h2o.logloss(drf.perf), '\n')
cat('RF Error:',h2o.mean_per_class_error(drf.perf), '\n')
cat('GBM Error:',h2o.mean_per_class_error(gbm.perf), '\n')
cat('DNN Error:',h2o.mean_per_class_error(dnn.perf), '\n')
cat('Ensmeble Error:',h2o.mean_per_class_error(ensemble.perf), '\n')
cat('RF Log-loss:',h2o.logloss(drf.perf), '\n')
cat('GBM Log-lossr:',h2o.logloss(gbm.perf), '\n')
cat('DNN Log-loss:',h2o.logloss(dnn.perf), '\n')
cat('Ensmeble Log-loss:',h2o.logloss(ensemble.perf), '\n')
# model parmeters
stack_ensemble <- h2o.stackedEnsemble(
model_id = "stack_ID",
x= x,
y = y,
training_frame = train,
validation_frame =valid,
base_models = list(best_drf,
best_dnn,
best_gbm),
metalearner_algorithm = "AUTO",
metalearner_nfolds = 10,
seed=123)
drf.perf<-h2o.performance(best_drf, newdata=test)             ## full test data
gbm.perf<-h2o.performance(best_gbm, newdata=test)             ## full test data
dnn.perf<-h2o.performance(best_dnn, newdata=test)             ## full test data
ensemble.perf<-h2o.performance(stack_ensemble, newdata=test)    ## full test data
cat('RF Error:',h2o.mean_per_class_error(drf.perf), '\n')
cat('GBM Error:',h2o.mean_per_class_error(gbm.perf), '\n')
cat('DNN Error:',h2o.mean_per_class_error(dnn.perf), '\n')
cat('Ensmeble Error:',h2o.mean_per_class_error(ensemble.perf), '\n')
cat('RF Log-loss:',h2o.logloss(drf.perf), '\n')
cat('GBM Log-lossr:',h2o.logloss(gbm.perf), '\n')
cat('DNN Log-loss:',h2o.logloss(dnn.perf), '\n')
cat('Ensmeble Log-loss:',h2o.logloss(ensemble.perf), '\n')
g.predict = as.data.frame(h2o.predict(object = stack_ensemble, newdata = grid))
h2o.shutdown(prompt=FALSE)
# Extract predicted landuse class
grid.xy$Class<-g.predict$predict
# Import lnaduse ID file
ID<-read.csv(paste0(dataFolder,".\\Sentinel_2\\Landuse_ID.csv"), header=T)
# Join landuse ID
grid.new<-join(grid.xy, ID, by="Class", type="inner")
# Omit missing values
grid.new.na<-na.omit(grid.new)
x<-SpatialPointsDataFrame(as.data.frame(grid.new.na)[, c("x", "y")], data = grid.new.na)
r <- rasterFromXYZ(as.data.frame(x)[, c("x", "y", "Class_ID")])
# Create color palette
myPalette <- colorRampPalette(c("light grey","burlywood4", "forestgreen","light green", "dodgerblue"))
# Plot Map
LU<-spplot(r,"Class_ID", main="Supervised Image Classification: Stack-Ensemble" ,
colorkey = list(space="right",tick.number=1,height=1, width=1.5,
labels = list(at = seq(1,4.8,length=5),cex=1.0,
lab = c("Road/parking/pavement" ,"Building", "Tree/buses", "Grass", "Water"))),
col.regions=myPalette,cut=4)
LU
# 1) create a data frame with just the features
features <- as.data.frame(train.hex) %>% dplyr::select(-Rate)
library(GWmodel)      ## GW models
library(plyr)         ## Data management
library(sp)           ## Spatial Data management
library(spdep)        ## Spatial autocorrelation
library(RColorBrewer) ## Visualization
library(classInt)     ## Class intervals
library(raster)       ## spatial data
library(grid)         ## plot
library(gridExtra)    ## Multiple plot
library(ggplot2)      #  plotting
library(tidyverse)    # data
library(SpatialML)    # Geographically weigted regression
# 1) create a data frame with just the features
features <- as.data.frame(train.hex) %>%  dplyr::select(-Rate)
knitr::include_graphics("E:\\GitHub\\geospatial-r-github\\Image\\empty_banner.png")
library(GWmodel)      ## GW models
library(plyr)         ## Data management
library(sp)           ## Spatial Data management
library(spdep)        ## Spatial autocorrelation
library(RColorBrewer) ## Visualization
library(classInt)     ## Class intervals
library(raster)       ## spatial data
library(grid)         ## plot
library(gridExtra)    ## Multiple plot
library(ggplot2)      #  plotting
library(tidyverse)    # data
library(SpatialML)    # Geographically weigted regression
# Define data folder
dataFolder<-"D:\\Dropbox\\Spatial Data Analysis and Processing in R\\Data_GWR\\"
county<-shapefile(paste0(dataFolder,"COUNTY_ATLANTIC.shp"))
state.bd<-shapefile(paste0(dataFolder,"STATE_ATLANTIC.shp"))
df<-read.csv(paste0(dataFolder,"data_all_1998_2012.csv"), header=T)
state <- list("sp.lines", as(state.bd, "SpatialLines"), col="grey50", lwd=.7,lty=3)
test.df<-df %>%
dplyr::select(FIPS, X, Y, Year, POVERTY, SMOKING, PM25, NO2, SO2, Rate) %>%
filter(Year == 2012)
valid.df<-df %>%
dplyr::select(FIPS, X, Y, Year, POVERTY, SMOKING, PM25, NO2, SO2, Rate) %>%
filter(Year == 2011)
train.df<-df %>%
dplyr::select(FIPS, X, Y,  Year, POVERTY, SMOKING, PM25, NO2, SO2, Rate) %>%
filter(Year == 2010)
test.df[, 5:9] = scale(test.df[, 5:9])
valid.df[, 5:9] = scale(valid.df[, 5:9])
train.df[, 5:9] = scale(train.df[, 5:9])
library(h2o)
h2o.init(nthreads = -1,max_mem_size ="48g",enable_assertions = FALSE)
test.mf<-test.df[, 5:10]
valid.mf<-valid.df[, 5:10]
train.mf<-train.df[, 5:10]
test.hex<-  as.h2o(test.mf)
valid.hex<-  as.h2o(valid.mf)
train.hex<-  as.h2o(train.mf)
response <- "Rate"
predictors <- setdiff(names(train.hex), response)
# 1) create a data frame with just the features
features <- as.data.frame(train.hex) %>%  dplyr::select(-Rate)
# 2) Create a vector with the actual responses
response <- as.data.frame(train.hex) %>% pull(Rate)
# 3) Create custom predict function that returns the predicted values as a vector
pred <- function(object, newdata)  {
results <- as.vector(h2o.predict(object, as.h2o(newdata)))
return(results)
}
